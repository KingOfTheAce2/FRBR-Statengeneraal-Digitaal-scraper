name: Crawl Statengeneraal Digitaal

on:
  schedule:
    # Runs daily at 02:00 UTC
    - cron: '0 2 * * *'
  workflow_dispatch: # Allows manual triggering

jobs:
  crawl-and-upload:
    runs-on: ubuntu-latest
    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Restore crawler state
        uses: actions/cache@v3
        with:
          path: |
            state.json
          key: state-json-${{ runner.os }}-${{ env.HF_DATASET_REPO }}
          restore-keys: state-json-

      - name: Run crawler script
        run: python crawler.py

      - name: Commit and push data
        run: |
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'github-actions@github.com'
          if [ -f sgd.jsonl ]; then
            git add sgd.jsonl state.json
            if [ -n "$(git status --porcelain)" ]; then
              git commit -m "Data: New batch from Statengeneraal Digitaal"
              git push
            else
              echo "No changes to commit."
            fi
          else
            echo "No data file found."
