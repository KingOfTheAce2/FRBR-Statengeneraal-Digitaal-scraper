# .github/workflows/sgd-crawler.yml

name: Crawl Statengeneraal Digitaal

on:
  schedule:
    # Runs daily at 02:00 UTC
    - cron: '0 2 * * *'
  workflow_dispatch: # Allows manual triggering

jobs:
  crawl-and-upload:
    runs-on: ubuntu-latest
    env:
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
      HF_PRIVATE: ${{ secrets.HF_PRIVATE || 'false' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip' # cache pip dependencies

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Restore crawl cache
        uses: actions/cache@v3
        with:
          path: visited.txt
          key: visited-${{ github.run_id }}
          restore-keys: visited-

      - name: Run crawler script
        run: python scripts/sgd_crawler.py --resume --workers 4

      - name: Commit and push changes
        run: |
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'github-actions@github.com'
          if ls data/*.jsonl >/dev/null 2>&1; then
            git add data/*.jsonl
            if [ -n "$(git status --porcelain)" ]; then
              git commit -m "Data: Add new batch from Statengeneraal Digitaal"
              git push
            else
              echo "No new data to commit."
            fi
          else
            echo "No batch files to commit."
          fi
